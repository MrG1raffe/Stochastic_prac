\documentclass[16pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}  
\usepackage{amsthm}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{bbm}

\newcommand{\incfig}[2]{%
    \def\svgwidth{#2 mm}
    \import{./figures/}{#1.pdf_tex}
}

\newtheorem{Th}{Теорема}
\newtheorem{St}{Утверждение}
\newtheorem{Lem}{Лемма}
\newtheorem{Def}{Определение}
\newtheorem*{Cons}{Следствие}
\newtheorem{Rem}{Замечание}
\newenvironment{Proof}{\par\noindent{\bf Доказательство.}}{\hfill$\scriptstyle\blacksquare$}

\DeclareMathOperator{\Arccos}{arccos}
\DeclareMathOperator{\Arctg}{arctg}
\DeclareMathOperator{\Cl}{Cl}
\DeclareMathOperator{\Sign}{sgn}
\DeclareMathOperator*{\Argmax}{Argmax}
\DeclareMathOperator*{\Var}{\mathbb{V}ar}
\DeclareMathOperator*{\Min}{min}


\newcommand\Real{\mathbb{R}} 
\newcommand\A{(\cdot)} 
\newcommand\Sum[2]{\sum\limits_{#1}^{#2}}
\newcommand\Scal[2]{\langle #1,\, #2 \rangle}
\newcommand\Norm[1]{\left\| #1 \right\|}
\newcommand\Int[2]{\int\limits_{#1}^{#2}}
\newcommand{\Me}{\mathbb{E}}
\newcommand{\I}{\mathbbm{1}}
\newcommand{\Prb}{\mathbb{P}}
\newcommand\Reff[1]{(\ref{#1})}
\newcommand{\deq}{\overset{d}{=}}
\newcommand{\Bor}{\mathcal{B}}

\newcommand\Pict[3]{
\begin{figure}[h!]
    \centering
    \incfig{#1}{#3}
    \caption{#2}
    \label{fig:#1}
\end{figure}
}

\begin{document}

\thispagestyle{empty}

\begin{center}
\ \vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчёт по практикуму}

\vspace{1cm}

{\LARGE\bfseries <<Стохастический анализ и моделирование>>}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 415 группы}\\
  Д.\,М.~Сотников

  \vspace{5mm}

  \textit{Руководитель практикума}\\
  А.\,Ю.~Заночкин
\end{flushright}

\vfill

\begin{center}
Москва, 2020
\end{center}

\newpage

\tableofcontents

\newpage 

\section{Задание 1}
\subsection{Постановка задачи}
\begin{enumerate}
	\item  Реализовать генератор схемы Бернулли с заданной вероятностью успеха $p$. Построить на его основе датчик биномиального распределения.
	\item Реализовать генератор геометрического распределения. Проверить для него свойство отсутствия памяти.
	\item Рассмотреть игру в орлянку --- бесконечную последовательность независимых испытаний с бросанием правильной монеты. Выигрыш $S_n$ определяется как сумма по $n$ испытаниям значений $1$ и $-1$ в зависимости от выпавшей стороны. Проиллюстрировать поведение $Y(i) = \dfrac{S_i}{\sqrt{n}}$ как функцию номера испытания $i = 1, \ldots, n$ для отдельно взятой траектории. Дать теоретическую оценку $Y(n)$ при $n \to \infty$.
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Моделирование распределений.}
Для моделирования бернуллиевской случайной величины с вероятностью успеха $p$ будем использовать следующий алгоритм:
\begin{itemize}
	\item Моделируем равномерно распределенную на $[0,\, 1]$ случайную величину $U \sim \mathcal{U}(0, 1)$.
	\item Если $U < p$, то датчик возвращает $X = 1$, В противном случае $X = 0$.
\end{itemize}

Будем моделировать биномиальное распределение $\mathrm{Bin}(n,\,p)$ как сумму $n$ случайных величин, имеющих распределение Бернулли с параметром $p$.

Геометрическое распределение $\mathrm{Geom}(p)$ можно моделировать с помощью схемы Бернулли с параметром $p$ как число <<неудач>> до первого <<успеха>>, однако этот метод требует работы с массивами случайной длины. Более эффективный подход дает следующее
\begin{St}
Пусть $\xi \sim \mathrm{exp}(\lambda), \ \lambda > 0$. Тогда $\lfloor \xi \rfloor \sim \mathrm{Geom}(p),$ где $p = 1 - e^{-\lambda}$.
\end{St}
\begin{Proof}

Функция распределения $\xi$ имеет вид
$F_\xi(x) = 1 - e^{-\lambda x}$. Поэтому для $n \in \mathbb{N}_0$
$$\Prb(\lfloor \xi \rfloor = n) = \Prb(n \leq \xi < n + 1) = e^{-n\lambda}(1 - e^{-\lambda}) = (1 - p)^np,$$
что и требовалось.
\end{Proof}

Таким образом, моделирование геометрического распределения с параметром $p$ сводится к моделированию экспоненциального распределения с параметром $\lambda = -\ln(1 - p).$

Экспоненциальное распределение легко моделируется при помощи метода обращения функции распределения:
$$F_\xi^{-1}(y) = -\dfrac{1}{\lambda}\ln(1 - y).$$

\begin{Th}
Пусть случайная величина $\xi$ имеет непрерывную и строго возрастающую функцию распределения $F_\xi\A$. Тогда $F^{-1}_\xi(U) \sim \xi, \ U \sim \mathcal{U}(0, 1)$.
\end{Th}
\begin{Proof}

Непрерывность и строгое возрастание $F_\xi\A$ гарантируют существование непрерывной и строго возрастающей обратной функции. 

Найдем функцию распределения случайной величины $F_\xi^{-1}(U)$:
$$\Prb(F_\xi^{-1}(U) < x) = \Prb(U < F_\xi(x)) = F_\xi(x),$$
поэтому $F_\xi^{-1}(U) \deq \xi$.
\end{Proof}

Эту теорему легко обобщить и на более общий случай \cite{Smirnov}, однако в рамках данного практикума приведенной формулировки будет достаточно.

\paragraph{Отсутствие последействия.}
\begin{Def}
Будем говорить, что распределение дискретной случайной величины $X$ обладает свойством отсутствия последействия, если
$\Prb(X > t + \tau | X \geq \tau) = \Prb(X > t)$ для любых $t, \tau \in \mathbb{Z}, \ t > 0$.
\end{Def}

У геометрического распределения $\Prb(X > t) = (1 - p)^{t + 1}$, поэтому
$$\Prb(X > t + \tau | X \geq \tau) = \dfrac{\Prb(X > t + \tau, X \geq \tau)}{\Prb(X \geq \tau)} = \dfrac{(1 - p)^{t + \tau + 1}}{(1 - p)^\tau} = \Prb(X > t).$$

Таким образом, геометрическое распределение обладает свойством отсутствия последействия. Более того, можно показать, что оно является единственным дискретным распределением, обладающим этим свойством. Поэтому проверка этого свойства для дискретного распределения позволяет определить, является ли оно геометрическим.

\paragraph{Игра в орлянку.}
Для анализа распределения $Y(i)$ из пункта 3. воспользуемся центральной предельной теоремой (далее ЦПТ):
\begin{Th}
Пусть $X_1, X_2, \ldots$ --- последовательность независимых случайных величин, имеющих конечное математическое ожидание $\mu$ и ненулевую дисперсию $\sigma^2$. Тогда
$$\dfrac{S_n - \mu n}{\sigma \sqrt{n}} \overset{d}{\to} \mathcal{N}(0, 1)\ \text{ при $n \to \infty$}.$$
\end{Th}

Ее доказательство можно найти в \cite{Shir}.

Поскольку при описанной игре в орлянку математическое ожидание равно $0$, а дисперсия равна $1$, распределение $Y(i)$ сходится к стандартному нормальному по ЦПТ. 

\subsection{Результаты работы программы}

\paragraph{Проверка датчика биномиального распределения.} На рис. 1 приведена гистограмма выборки, полученной с помощью построенного датчика. По ЦПТ такое распределение аппроксимируется нормальным с параметрами $\mu = np,\, \sigma^2 = np(1-p)$.
\begin{figure}[h]
	\center
    \includegraphics[scale=0.7]{1_1.pdf}
    \caption{Моделирование биномиального распределения с $n = 10^4,\, p = 0.5$.}
\end{figure}

\paragraph{Проверка отсутствия памяти.} Обозначим $$\xi \sim \mathrm{Geom}(p), \quad \eta = (\xi | \xi \geq \tau) - \tau.$$ 

В случае, когда распределение обладает свойством отсутствия последействия, $\xi$ и $\eta$ должны быть одинаково распределены. Рис. 2 показывает совпадение гистограмм и эмпирических функций распределения этих случайных величин при $p = 0.4,\, \tau = 3$.
\begin{figure}[h]
	\center
    \includegraphics[scale=0.5]{1_2.pdf}
    \hfill
    \includegraphics[scale=0.5]{1_3.pdf}
    \caption{Сравнение гистограмм и выборочных функций распределения $\xi$ и $\eta$.}
\end{figure}

\paragraph{Моделирование игры в орлянку.}
На рис. 3 представлены 10 траекторий случайной последовательности $Y(i) = \dfrac{S_i}{n}$. На рис. 4 построена гистограмма для выборки случайных величин $Y(n)$ при $n = 10^4$, подтверждающая теоретический результат.

\begin{figure}[h]
	\center
    \includegraphics[scale=0.7]{1_4.pdf}
    \caption{Траектории $Y\A$ при игре в орлянку.}
\end{figure}

\begin{figure}[h]
	\center
    \includegraphics[scale=0.7]{1_5.pdf}
    \caption{Выборка случайных величин, совпадающих по распределению с $Y(10^4)$.}
\end{figure}

\newpage
\section{Задание 2}
\subsection{Постановка задачи}
\begin{enumerate}
\item Построить датчик сингулярного распределения, имеющий в качестве функции распределения канторову лестницу. С помощью критерия Колмогорова убедиться в корректности работы датчика.
\item Для канторовых случайных величин проверить свойство симметричности относительно 0.5 и самоподобия относительно деления деления на 3 с помощью критерия Смирнова.
\item Вычислить значение матожидания и дисперсии для данного распределения. Сравнить теоретические и эмпирические значения, проиллюстрировать сходимость.
\end{enumerate}

\subsection{Теоретическая часть}

\paragraph{Моделирование сингулярного распределения.}

Для моделирования требуемого распределения воспользуемся тем, что носителем распределения является канторово множество, то есть числа, не имеющие единиц в троичной записи. Заметим также, что если $k$ разрядов троичной записи случайной величины уже известны, то $(k+1)$-й разряд принимает одно из значений $0, 2$ с одинаковой вероятностью. Это следует из самоподобия канторовой лестницы. Поэтому будем рассматривать схему Бернулли размера $n$, состоящую из разрядов числа в троичной системе исчисления (при <<успехе>> разряд равен 2) с таким n, чтобы ошибка в вычислении числа не превосходила $\varepsilon$.

Так как ошибка в худшем случае равна
$ \sum\limits_{k=n+1}^\infty\left(2/3\right)^{k} = 3^{-n},$
достаточно взять $n \geq \log_3 \dfrac{1}{\varepsilon}$.

\paragraph{Критерий Колмогорова.}
Сформулируем теорему Колмогорова \cite{Kolm}. 
\begin{Th}
Пусть $X_1, \ldots, X_n$ --- выборка независимых случайных величин, имеющих функцию распределения $F\A$. Построим по ней выборочную функцию распределения $F_n\A$. Тогда $$\sup_{x \in \Real}|F_n(x) - F(x)| \overset{d}{\to} K,$$
где $K$ --- распределение Колмогорова.
\end{Th}

Опишем алгоритм применения {критерия Колмогорова}. Будем проверять гипотезу
$$H_0\colon \ F(\cdot) = F_0(\cdot),$$
где $F_0(\cdot)$ - функция Кантора. Для этого вычислим
$$D_n = \sup_x|F_n(x) - F_0(x)|,$$
где $F_n(\cdot)$ --- эмпирическая функция распределения, построенная по выборке размера $n$. Положим $\gamma = K^{-1}(1-\alpha)$. Здесь $\alpha$ - уровень значимости, а $K^{-1}(x)$ --- квантиль распределния Колмогорова порядка $x$. 

Будем отвергать гипотезу в случае, когда
$$\sqrt{n}D_n \geq \gamma.$$

Положим $\alpha = 0.05$. Тогда из таблицы распределения Колмогорова $\gamma = K^{-1}(0.95) = 1.36$.

\paragraph{Критерий Смирнова.} Для того, чтобы по двум выборкам проверить, имеют ли они одинаковое распределение, используется {критерий Смирнова}. 

Рассматриваются $X = (x_1, \ldots, x_n),\, Y = (y_1, \ldots, y_m)$ --- выборки, элементы которых имеют функции распределения $F$ и $G$ соответственно, а $F_n, G_m$ --- построенные по ним эмпирические функции распределения. Обозначим 
$$D_{mn} = \sup_{x \in \mathbb{R}}|F_n(x) - G_n(x)|.$$
Теорема Смирнова \cite{Smir} утверждает, что
$$\mathbb{P}\left(\sqrt{\frac{mn}{m+n}}D_{mn} < x\right) \to K(x).$$
Будем отвергать гипотезу
$$H_0\colon \ F(\cdot) = G(\cdot),$$
если 
$\sqrt{\dfrac{mn}{m+n}}D_{mn} > \gamma, \quad \gamma = K^{-1}(1 - \alpha).$

Как и в прошлом пункте, $\alpha = 0.05,\, \gamma = 1.36$.

Для проверки симметричности будем проверять гипотезу
$$H_0\colon X \overset{d}{=} 1 - X.$$
с помощью критерия Смирнова, моделируя выборки с помощью построенного датчика.

Проверка самоподобия сводится к проверке гипотезы
$$H_0\colon \dfrac{Y}{3} \overset{d}{=} \left(Y \, \bigg| \, Y \leq \dfrac13\right).$$

\paragraph{Моменты сингулярного распределения.}
В силу симметричности распределения относительно $\dfrac12$, $\mathbb{E}X = \dfrac12$.
Условия симметричности и самоподобия можно переписать в виде
$$F(x) = 1 - F(1 - x), \quad F\left(\frac{x}{3}\right) = \frac12 F(x), \ x \in [0, \, 1].$$
Вычислим второй момент, пользуясь этими свойствами:
$$\mathbb{E}X^2 = \int_0^1x^2dF(x) = \int_0^{\frac13}x^2dF(x)+\int_\frac23^1 x^2dF(x).$$
После замен переменных, переводящих множество интегрирования в отрезок $[0,\, 1]$, используя свойства симметричности и самоподобия, получим
$$\mathbb{E}X^2 = \frac13 + \frac19\mathbb{E}X^2, \quad \mathbb{E}X^2 = \frac38,$$
откуда легко найти $\Var \mathrm{X} = \dfrac18.$

Оценим по выборке математическое ожидание и дисперсию, используя статистики $$\overline{x} = \dfrac{1}{n}\sum\limits_{k=1}^{n}x_k, \quad {\hat \sigma}^2 = \dfrac{1}{n}\sum\limits_{k=1}^{n}(x_k - \overline{x})^2.$$
\subsection{Результаты работы программы}

\paragraph{Проверка датчика.}
Рис. 5 демонстрирует работу построенного датчика сингулярного распределения. Размер выборки $n = 1000$. 

\begin{figure}[h]
	\center
    \includegraphics[scale=0.7]{2_1.pdf}
    \caption{Работа датчика сингулярного распределения.}
\end{figure}

\paragraph{Проверка гипотез.}
Проверим построенные датчик с помощью критерия Колмогорова с уровнем значимости $\alpha = 0.05$ и размером выборки $n = 1000$. При моделировании проверки гипотезы $m = 1000$ раз, нулевая гипотеза отклонялась в $4.5 \%$  случаев.

При проверке симметричности и самоподобия с помощью критерия Смирнова с теми же параметрами, нулевая гипотеза отклонялась ровно в $5\%$ случаев для обеих гипотез.

\paragraph{Оценка моментов.} На рис. 6 показана сходимость выборочных математического ожидания и дисперсии к найденным теоретически показателям.

\begin{figure}[h]
	\center
    \includegraphics[scale=0.5]{2_2.pdf}
    \hfill
    \includegraphics[scale=0.5]{2_3.pdf}
    \caption{Оценка моментов сингулярного распределения.}
\end{figure}

\newpage
\section{Задание 3}
\subsection{Постановка задачи}
\begin{enumerate}
\item Построить датчик экспоненциального распределения. Проверить для данного распределения свойство отсутствия памяти. Пусть $X_1, \ldots, X_n$ независимы и имеют экспоненциальное распределение с параметрами $\lambda_1, \ldots, \lambda_n$. Найти распределение случайной величины $Y = \min(X_1, \ldots, X_n)$.
\item На основе датчика экспоненциального распределения построить датчик пуассоновского распределения.
\item Построить датчик пуассоновского распределения как предел биномиального. С помощью критерия хи-квадрат убедиться, что получен датчик распределения Пуассона.
\item Построить датчик стандартного нормального распределения методом моделирования случайных величин с переходом в полярные координаты. Проверить при помощи t-критерия Стьюдента равенство матожиданий, а при помощи критерия Фишера равенство дисперсий.
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Экспоненциальное распределение.}
Датчик экспоненциального распределения уже был построен в задании 1. Свойство отсутствия последействия формулируется и доказывается в этом случае аналогично тому, как это было сделано для геометрического распределения.

Найдем теперь распределение $Y = \min(X_1, \ldots, X_n)$:
\begin{multline}\notag
F_Y(x) = \mathbb{P}(\min(X_1, \ldots, X_n) < x) = 1 - \mathbb{P}(X_1 \geq x, \ldots, X_n \geq x) =\\= 1 - \prod_{i=1}^n(1- F_{X_i}(x)) = 1 -\exp\left(-\sum\limits_{i=1}^n \lambda_i x\right),
\end{multline}
откуда $Y \sim \mathrm{exp}\left(\sum_{k=1}^n \lambda_k\right)$.

\paragraph{Моделирование распределения Пуассона.}
Расмотрим пуассоновский процесс $X(t)$ с интенсивностью $\lambda$. Так как пуассоновский процесс является марковским, время между его скачками имеет экспоненциальное распределение с параметром $\lambda$. При этом $X(1)$ имеет распределение Пуассона с параметром $\lambda$. Таким образом, при моделировании $\xi_k \sim \exp(\lambda)$ до тех пор, пока
$$\sum_{k=1}^\eta \xi_k > 1,$$
величина $\eta - 1$, равная числу скачков до момента $t = 1$, будет иметь требуемое пуассоновское распределение.

Другой подход к построению датчика дает теорема Пуассона (доказательство в \cite{Shir}).
\begin{Th}
Рассмотрим схему серий испытаний Бернулли с вероятностями успеха $p_n$. Тогда, если $$p_n \underset{n \to \infty}{\to} 0, \quad 
np_n \underset{n \to \infty}{\to} \lambda > 0,$$
число успехов $\mu_n$ в схеме Бернулли сходится по распределению к пуассоновскому распределению с параметром $\lambda$.

При этом верна оценка
$$\left|\mathbb{P}(\nu_n \in A) - \sum_{k \in A}e^{-\lambda}\dfrac{\lambda^k}{k!}\right| \leq np^2 = \lambda p.$$
\end{Th}

Потребуем, чтобы вероятности отличались не более, чем на $\varepsilon$. Тогда биномиальное распределение с параметрами $p = \dfrac{\varepsilon}{\lambda}, n = \dfrac{\lambda}{p}$ аппроксимирует требуемое распределение пуассона с заданной точностью.

\paragraph{Критерий $\chi^2$ Пирсона.} 
Критерий $\chi^2$ используется для проверки гипотезы
$$H_0\colon F\A = F_0\A,$$
где $F_0\A$ --- заданная функция распределения. Разобьем $\Real$ на непересекающиеся множества $\Delta_1, \ldots, \Delta_m$. Обозначим $p_i = \Prb(\Delta_i | H_0)$, $N_i$ --- число элементов выборки $X_1, \ldots, X_n$, попавших в $\Delta_i$. Тгда статистика 
$$Q = \Sum{i=1}{m}\dfrac{(N_i - np_i)^2}{np_i}$$
имеет\footnote{Формулировки всех утверждений, относящихся к проверке гипотез, приведены в \cite{DeGroot}} распределение $\chi^2(m - 1)$.

Будем отклонять $H_0$, если $Q \geq \gamma$, где $\gamma = \chi^2_{m-1}(1 - \alpha)$, а $\chi^2_{m-1}(x)$ --- квантиль порядка $x$ распределения хи-квадрат с $m - 1$ степенью свободы. Построенный таким образом критерий имеет уровень согласия $\alpha$.

\paragraph{Моделирование нормального распределения.}
Ясно, что достаточно построить датчик $X$ стандартного нормального распределения. Нормальное распределение с параметрами $\mu,\, \sigma^2$ получается из него преобразованиями сдвига и масштаба $\mu + \sigma X$.

Стандартное нормальное распределение будем генерировать полярным методом: рассмотрим двумерную случайную величину $(X_1,\, X_2)$, координаты которой имеют стандартное нормальное распределение. Найдем распределения полярных  координат. Очевидно, что в силу симметрии угол $\phi$ имеет равномерное распределение на отрезке $[0,\, 2\pi]$. Квадрат радиуса по определению имеет распределение $\chi^2$ с двумя степенями свободы, которое совпадает с $\mathrm{exp}\left(\frac12\right)$. Таким образом, полярные координаты моделируются доступными датчиками распределений
$$r^2 \sim \mathrm{exp}\left(\frac12\right), \quad \phi \sim \mathcal{U}(0, 2\pi),$$
откуда легко перейти к декартовым координатам
$$x = r \cos\phi \sim \mathcal{N}(0, 1), \quad y = r \sin \phi \sim \mathcal{N}(0, 1).$$

\paragraph{Критерий Стьюдента.}
Рассматривается выборка $X_1, \ldots, X_n$ из нормального распределения с неизвестными параметрами. Критерий Стьюдента позволяет проверить гипотезу
$$H_0\colon\ \mathbb{E}X = \mu.$$

Для оценки стандартного отклонения используется оценка
$$\sigma' = \sqrt{\dfrac{1}{n-1}\sum_i(X_i - \bar X)^2}.$$
Тогда статистика
$$U = \sqrt{n}\frac{\bar X - \mu}{\sigma'}.$$
имеет распределение Стьюдента с $n - 1$ степенью свободы (при условии истинности $H_0$).

Для построения критерия с уровнем значимости $\alpha$ будем отклонять $H_0$,
когда $|U| \geq \gamma$, где $\gamma = T_{n - 1}^{-1}\left(1 - \frac{\alpha}{2}\right).$

\paragraph{Критерий Фишера.}
Критерий Фишера используется для сравнения дисперсий двух нормальных выборок с неизвестными параметрами. Точнее, будем проверять гипотезу
$$H_0\colon \sigma_1 = \sigma_2,$$
где $\sigma_1, \, \sigma_2$ --- стандартные отклонения нормальных распределений, из которых взяты первая и вторая выборки соответственно.

Рассмотрим выборки $X = (X_1, \ldots, X_n), \ Y = (Y_1, \ldots, Y_m)$. Для них статистика
$$ V = \dfrac{m-1}{n-1}\dfrac{\sum_i(X_i - \bar X)^2}{\sum_i(Y_i - \bar Y)^2}$$ 
имеет распределение Фишера с параметрами $n - 1$ и $m - 1$.

Будем отклонять $H_0$, если $V \leq \gamma_1$ или $V \geq \gamma_2$. Здесь
$$\gamma_1 = F_{n-1, m-1}^{-1}\left(\frac{\alpha}{2}\right), \quad
\gamma_2 = F_{n-1, m-1}^{-1}\left(1 - \frac{\alpha}{2}\right),$$  а $F_{n-1, m-1}^{-1}(x)$ --- квантиль F-распределения с параметрами $n - 1$ и $m - 1$ порядка $x$.
\subsection{Результаты работы программы}

\paragraph{Отсутствие последействия экспоненциального распределения.} 
Как и в задании 1,
$$\xi \sim \mathrm{Exp}(\lambda), \quad \eta = (\xi | \xi \geq \tau) - \tau.$$ 

На рис. 7 показано совпадение гистограмм и выборочных функций распределения при $\lambda = 0.5, \, \tau = 3$.
\begin{figure}[h]
	\center
    \includegraphics[scale=0.5]{3_1.pdf}
    \hfill
    \includegraphics[scale=0.5]{3_2.pdf}
    \caption{Сравнение гистограмм и выборочных функций распределения $\xi$ и $\eta$.}
\end{figure}

\paragraph{Распределение $\min(X_1,\ldots, X_n)$.} 
На рис. 8 показано эмпирическое распределение минимума трех случайных величин $X_1,\, X_2,\, X_3$, имеющих экспоненциальные распределения с параметрами $\lambda_1 = 2,\, \lambda_2 = 1,\, \lambda_3 = 4$. Зеленая линия на графике --- плотность экспоненциального распределения с параметром $\lambda = 7$.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{3_3.pdf}
	\caption{Распределение $Y = \min(X_1, \ldots, X_n)$.}
\end{figure}

\paragraph{Критерий Пирсона.} Во всех статистических критериях в этом задании будем брать уровень значимости $\alpha = 0.05$, размер выборки $n = 1000$. Процесс проверки гипотезы будет моделироваться $m = 1000$ раз. Для применения критерия Пирсона для распределения Пуассона с $\lambda = 4$, разобьем множество целых неотрицательных чисел на подмножества
$$\Delta_1 = \{0, \ 1\}, \Delta_2 = \{2\}, \Delta_3 = \{3\}, \Delta_4 = \{4\}, \Delta_5 = \{5\}, \Delta_6 = \{6\}, \Delta_7 = \{k \geq 7\}.$$

Вычислим вероятности $p_i = \mathbb{P}(\Delta_i)$ попадания в каждое множество
$$p_1 \approx 0.0916, \ p_2 \approx 0.1465, \ p_3 \approx 0.1954,\  p_4 \approx 0.1954, \ 
\ p_5 \approx 0.1563, \ p_6 \approx 0.1042, \ p_7 \approx 0.1107. $$

Множества $\Delta_i$ выбраны таким образом, чтобы теоретические вероятности попадания в них имели один порядок.

При моделировании критерия Пирсона нулевая гипотеза была отклонена в $4.3\%$ случаев.

\paragraph{Работа полярного метода} Результат работы датчика нормального распределения, использующего полярный метод, показан на гистограмме на рис. 9.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{3_4.pdf}
	\caption{Моделирование стандартного нормального распределения полярным методом.}
\end{figure}

\paragraph{Критерий Стьюдента.} С помощью датчика нормального распределения с $\mu = 1,\, \sigma^2 = 4$ была сгенерирована выборка. Гипотеза
$$H_0: \mu = 1$$
отклонялась критерием Стьюдента в $3.1\%$ случаев.

\paragraph{Критерий Фишера.} Для тех же параметров $\mu = 1,\, \sigma^2 = 4$ гипотеза
$$H_0: \sigma^2 = 4$$
отклонялась критерием Фишера в $5.6\%$ случаев.



\newpage
\section{Задание 4}
\subsection{Постановка задачи}
\begin{enumerate}
\item Построить датчик распределения Коши.
\item На основе датчика распределения Коши с помощью метода фон Неймана построить датчик стандартного нормального распределения. Убедиться в корректности построенного датчика и обосновать наблюдаемую линейную зависимость.
\item Сравнить скорость моделирования стандартного нормального распределения в заданиях 3 и 4.
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Моделирование распределения Коши.}
Функция распределения случайной величины, имеющей распределение Коши с параметром сдвига $x_0$ и параметром масштаба $\gamma$, имеет вид
$$F(x) = \frac{1}{\pi} \arctan\left(\frac{x-x_0}{\gamma}\right) + \frac12.$$

Обратная к ней
$$F^{-1}(y) = x_0 + \gamma \tan\left(\pi\left(y - \frac12\right)\right).$$

Будем использовать ее для моделирования распределения Коши:
$$\xi = F^{-1}(U) \sim \mathcal{C}(x_0, \gamma), \quad U \sim \mathcal{U}(0, 1).$$

\paragraph{Метод элиминации фон Неймана.}
Пусть имеется датчик распределения с плотностью $g(\cdot)$ (в этой задаче 
$g(x) = \frac{\gamma}{\pi[(x - x_0)^2 + \gamma^2]}$), и требуется смоделировать случайную величину с плотностью $f(\cdot)$ (в нашем случае $f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}})$, причем
$$\sup_{x \in \Real} \frac{f(x)}{g(x)} \leq k.$$

Будем использовать следующий алгоритм:
\begin{enumerate}
	\item Моделируем $X \sim g\A$ и равномерно распределенную на единичном отрезке случайную  величину $U$.
	\item Если $\dfrac{f(X)}{g(X)} \geq kU,$
то $Y = X$, и процесс завершается, в противном случае нужно вернуться у шагу 1.
\end{enumerate} 

Докажем, что полученная таким образом случайная величина $Y$ имеет требуемое распределение с плотностью $f\A$.
Обозначим $A = \left\{\omega\colon \dfrac{f(X)}{g(X)} \geq kU\right\}$.
$$\Prb(A) = \int\limits_\Real\Int{0}{\frac{f(x)}{kg(x)}}g(x)dudx = \dfrac{1}{k}\int\limits_\Real f(x)dx = \dfrac{1}{k}.$$

Условная плотность $(X, U)$ при условии $A$ имеет вид
$p(x, u | A) = kg(x)$, если $A$ выполнено.

Тогда для произвольного борелевского $B$
$$\Prb(X \in B | A) = \int\limits_B\Int{0}{\frac{f(x)}{kg(x)}}kg(x)dudx = \int\limits_B f(x) dx,$$
что и требовалось доказать.

Нетрудно показать, что наилучшее приближение стандартного нормального распределения распределением Коши имеет параметры $x_0 = 0, \gamma = 1$. При этом
$$k = \sup_x \frac{f(x)}{g(x)} = \sqrt{\dfrac{2\pi}{e}}.$$

\paragraph{Обоснование линейной зависимости в normplot.}
Для стандартного нормального распределения точки лежат на прямой по построению: ось ординат преобразуется так, чтобы функция нормального распределения была линейной. Таким образом, если $\xi \sim \mathcal{N}(0, 1)$, то $F(x) = \mathbb{P}(\xi < x)$ линейна на таком графике. Произвольную нормальную случайную величину можно представить в виде $\eta = \sigma \xi + \mu$, и для нее
$$\mathbb{P}(\eta < x) = \mathbb{P}\left(\xi < \frac{x - \mu}{\sigma}\right)$$
также будет линейной на этом графике. При этом происходит сдвиг на $\mu$ по оси абсцисс, а угловой коэффициент умножается на $\dfrac{1}{\sigma}$.

\paragraph{Сравнение скорости моделирования нормального распределения.} Для сравнения скорости работы построенных датчиков, создадим логарифмическую сетку для размеров выборки размера $n$. Для выборки каждого размера будем запускать датчик $m$ раз, а затем в качестве оценки времени работы брать медиану измерений как устойчивую к большим выбросам оценку среднего.
\subsection{Результаты работы программы}

\paragraph{Датчик распределения Коши} На рис. 10 показана работа датчика, моделирующего стандартное распределение Коши.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{4_1.pdf}
	\caption{Моделирование стандартного распределения Коши.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.5]{4_2.pdf}
	\hfill
	\includegraphics[scale=0.5]{4_3.pdf}
	\caption{normplot для $\mathcal{N}(0, 1)$ и $\mathcal{N}(2, 9)$.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{4_4.pdf}
	\caption{normplot для стандартного распределения Коши.}
\end{figure}

\paragraph{Графики normplot.} На рис. 11 изображены графики normplot для стандартного нормального распределения (слева) и для нормального распределения с параметрами $\mu = 2,\, \sigma^2 = 3$. Рис. 12 показывает, что линейная зависимость пропадает для выборки из распределения, не являющегося нормальным. Также из графика видно, что распределение Коши имеет значительно более тяжелые хвосты, нежели нормальное.

\paragraph{Сравнение времени работы датчиков.} На рис. 13 показана зависимость времени работы датчиков  от размера выборки. Сравниваются датчики, использующие полярный метод и метод элиминации фон Неймана, а также стандартный датчик, реализованный в библиотеке numpy. Каждый датчик запускается $m = 100$  раз, среднее время оценивается с помощью медианы. Таким образом редкие большие выбросы, возможные при работе компьютера, не оказывают сильного влияния на среднюю величину.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{4_5.pdf}
	\caption{Сравнение времени работы датчиков.}
\end{figure}

\newpage
\section{Задание 5}
\subsection{Постановка задачи}
\begin{enumerate}
\item Пусть $X_i \sim \mathcal{N}(\mu, \sigma^2)$. Убедиться эмпирически в справедливости ЗБЧ и ЦПТ: исследовать поведение суммы $S_n$ и эмпирического распределения величины
$$\sqrt{n}\left(\frac{S_n}{n} - \mu\right).$$
\item Считая $\mu$ и $\sigma$ неизвестными, для пункта 1 построить доверительные интервалы для среднего и дисперсии.
\item Пусть $X_i \sim \mathcal{C}(a, b)$ имеет распределение Коши со сдвигом $a$ и масштабом $b$. Проверить эмпирически, как ведут себя суммы $\frac{S_n}{n}$. Результат объяснить, а также найти распределения данных сумм.
\end{enumerate}

\subsection{Теоретическая часть}
ЦПТ уже была сформулирована как теорема 2. Приведем теперь формулировку усиленного закона больших чисел (далее УЗБЧ) в форме Колмогорова. Его доказательство можно найти, например, в \cite{Shir}.
\begin{Th}
Пусть $X_1, X_2, \ldots$ --- последовательность независимых одинаково распределенных случайных величин, имеющих конечные первые мометы. Тогда
$$\dfrac{1}{n}\Sum{i=1}{n}X_i \overset{\text{{\rm п.н.}}}{\to} \Me X_1.$$ 
\end{Th}

По ЗБЧ величина $S_n$ должна вести себя асимптотически как $n\Me X_1$, а случайная величина $\sqrt{n}\left(\dfrac{S_n}{n} - \mu\right)$ сходится по распределению к $\mathcal{N}(0, \sigma^2)$. 

\paragraph{Доверительный интервал для математического ожидания.}
Так как статистика $$U = \sqrt{n}\frac{\bar X - \mu}{\sigma'}, \quad \sigma' = \sqrt{\dfrac{\sum_{i=1}^n(X_i - \bar X)^2}{n-1}}$$
имеет \cite{DeGroot} распределение Стьюдента с $n - 1$ степенью свободы, можно построить доверительный интервал с уровнем доверия $\alpha$ следующим образом:
$$\mathbb{P}(\gamma_1 < U < \gamma_2) = \mathbb{P}\left(\bar X - \frac{\sigma'\gamma_2}{\sqrt{n}} < \mu < \bar X - \frac{\sigma'\gamma_1}{\sqrt{n}}\right) = \alpha.$$
Значения $\gamma_1$ и $\gamma_2$ выберем так, чтобы интервал был симметричным:
$$\gamma_1 = T^{-1}_{n-1}\left(\frac{1 - \alpha}{2}\right),\  \gamma_2 = T^{-1}_{n-1}\left(\frac{1 + \alpha}{2}\right).$$
Итак, доверительный интервал для параметра $\mu$ имеет вид
$$\left(\bar X - \frac{\sigma'\gamma_2}{\sqrt{n}}, \ \bar X - \frac{\sigma'\gamma_1}{\sqrt{n}}\right).$$

\paragraph{Доверительный интервал для дисперсии.}
Для оценки дисперсии будем использовать статистику
$$V = \frac{\sum_i(X_i - \bar X)^2}{\sigma^2} \sim \chi^2(n - 1).$$
$$\mathbb{P}(\gamma_1 < V < \gamma_2) = \mathbb{P}\left(\frac{\sum_i(X_i - \bar X)^2}{\gamma_2} < \sigma^2 < \frac{\sum_i(X_i - \bar X)^2}{\gamma_1}\right) = \alpha.$$
Для построения симметричного интервала
$$\gamma_1 = \chi_{n-1}^{-1}\left(\frac{1-\alpha}{2}\right), \ \gamma_2 = \chi_{n-1}^{-1}\left(\frac{1+\alpha}{2}\right).$$
Доверительный для $\sigma^2$ с уровнем доверия $\alpha$ имеет вид
$$\left(\frac{\sum_i(X_i - \bar X)^2}{\gamma_2}, \ \frac{\sum_i(X_i - \bar X)^2}{\gamma_1}\right).$$

\paragraph{Поведение сумм распределения Коши.}
Найдем распределение $\dfrac{S_n}{n}$ с помощью характеристической функции распределения Коши:
$$\varphi(t) = \mathbb{E}e^{it\xi} = \frac1\pi\int\frac{be^{itx}}{(x - a)^2 + b^2}dx = \frac{e^{ita}}{\pi}\int\frac{e^{itbx}}{x^2 + 1}dx = e^{ita - |t|b}.$$
Тогда характеристическая функция $\dfrac{S_n}{n}$ имеет вид
$$\varphi_{\frac{S_n}{n}}(t) = \left[\varphi\left(\frac{t}{n}\right)\right]^n = \varphi(t),$$
поэтому $\dfrac{S_n}{n} \sim \mathcal{C}(a, b)$.

\subsection{Результаты работы программы}
\paragraph{Иллюстрация ЗБЧ.} Рис. 14 демонстрирует сходимость нормированных сумм к среднему при проведении дополнительных испытаний.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{5_1.pdf}
	\caption{Поведение нормированных сумм.}
\end{figure}

\paragraph{Иллюстрация ЦПТ.} На рис. 15 показанна гистограмма выборки случайной величины $\sqrt{n}\left(\frac{S_n}{n} - \mu\right)$, а также плотность теоретического предела данной случайной последовательности.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{5_2.pdf}
	\caption{Выборка из $\sqrt{n}\left(\frac{S_n}{n} - \mu\right)$ при $n = 10^4$.}
\end{figure}

\paragraph{Доверительные интервалы.} На рис. 16 изображены доверительные интервалы с уровнем доверия $\alpha = 0.95$ для параметров $\mu,\, \sigma^2$, построенные в соответствии с теоретической частью. При построении по выборке размера $n = 1000$ доверительных интервалов $m = 1000$ раз, истинный параметр оказывался в интервале в $95.4\%$ случаев для математического ожидания и в $95.3\%$ случаев для дисперсии.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.5]{5_3.pdf}
	\hfill
	\includegraphics[scale=0.5]{5_4.pdf}
	\caption{Доверительные интервалы для $\mu$ и $\sigma^2$. Истинные значения $\mu = 0,\, \sigma = 2$.}
\end{figure}

\paragraph{Поведение сумм случайных величин, имеющих распределение Коши.} На рис. 17 изображено поведение нормированной суммы, каждое слагаемое которой имеет распределение Коши с параметром сдвига $1$ и параметром масштаба, равным $2$. Из-за того, что отдельные элементы вносят значительный вклад в общую сумму, сходимость отсутствует, и распределение нормированной суммы является распределением Коши с теми же параметрами.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.5]{5_5.pdf}
	\hfill
	\includegraphics[scale=0.5]{5_6.pdf}
	\caption{<<ЗБЧ>> для распределения Коши и распределение $\dfrac{S_n}{n}$ при $n = 10^4$.}
\end{figure}

\newpage
\section{Задание 6}
\subsection{Постановка задачи}
\begin{enumerate}
\item Посчитать интеграл
$$J = \int\limits_{-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}\ldots\int\limits_{-\infty}^{+\infty} \dfrac{e^{-\left(x_1^2 + \ldots + x_{10}^2 + \frac{1}{2^7x_1^2\ldots x_{10}^2}\right)}}{x_1^2\ldots x_{10}^2}dx_1dx_2\ldots dx_{10}$$
\begin{itemize}
	\item методом Монте--Карло;
	\item методом квадратур, сводя задачу к вычислению собственного интеграла Римана.
\end{itemize} 
\item Для каждого случая оценить точность вычислений.
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Метод Монте--Карло.} Отметим, что случайный вектор $X = (X_1, \ldots, X_{10})$, у которого каждая компонента распределена нормально с $\mu = 0,\ \sigma^2 = \frac12$, имеет плотность
$$p(x) = \frac{1}{\pi^5}e^{-(x_1^2 + \ldots + x_{10}^2)}, \quad x \in \Real^{10}.$$
Перепишем интеграл в виде
$$ J = \pi^5 \int\limits_{\mathbb{R}^{10}} ye^{-\frac{y}{128}}p(x)dx, \quad y = \left(\prod_{k=1}^{10} x_k\right)^{-2}$$
и заметим, что он равен математическому ожиданию
$\mathbb{E}f(Y),$
где $f(y) = \pi^5 y e^{-\frac{y}{128}},\ Y = \left(\prod_{k=1}^{10} X_k\right)^{-2}$, а $X_k \sim \mathcal{N}\left(0, \frac12\right)$ и независимы.

По усиленному закону больших чисел $$\dfrac1n \Sum{k=1}{n} f(Y_k) \overset{\text{п.н.}}{\rightarrow} \mathbb{E}f(Y) = J.$$

\paragraph{Оценка точности.} Для метода Монте--Карло найдем ошибку $\varepsilon$, вероятность превзойти которую при моделировании выборки размера $n$ не превосходит $\alpha$. Воспользуемся для этого неравентсвом Чебышёва:
$$\mathbb{P}\left(\left|\dfrac{1}{n}\sum_{k=1}^nf(Y_k) - \mathbb{E}f(Y)\right| \geq \varepsilon \right) \leq \dfrac{\Var [f(Y)]}{n\varepsilon^2} = \alpha.$$

Дисперсию $f(Y)$ будем аппроксимировать выборочной дисперсией $\dfrac{1}{n}\sum\limits_i \left(f(Y_i) - \overline{f(Y)}\right)^2$, которая является состоятельной оценкой.

\paragraph{Метод квадратур.} Сделаем в интеграле замену 
$$x_k = \tan\left(y_k\right), \quad dx_k = \dfrac{dy_k}{\cos^2\left(y_k\right)}, \ k = 1, \ldots, 10.$$
После замены и некоторых упрощений интеграл примет вид
$$\int\limits_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\ldots\int\limits_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \exp \left\{-\sum\limits_k \tan^2(y_k) - \dfrac{1}{128}\prod\limits_k \cot^2 (y_k)\right\}\prod\limits_k \dfrac{1}{\sin^2(y_k)}dy_1\ldots dy_{10}$$

Обозначим подынтегральную функцию $f(y_1, \ldots, y_{10})$ и заметим, что она симметрична и четна по всем аргументам.

Построим равномерную сетку размера $m$ на каждом отрезке $\left[-\dfrac{\pi}{2}, \dfrac{\pi}{2}\right]$:
$$x_i = -\frac{\pi}{2} + h(i + 0.5), \quad i = 0, \ldots, m-1,$$
В силу четности $f$ можно вычислить интеграл на $\left[0, \dfrac{\pi}2\right]$ по каждому измерению, и затем умножить результат на $2^{10}$.

$h = \dfrac{\pi}{m}$ --- шаг сетки. Элементы сетки --- центры отрезков, полученных при разбиении $\left[-\dfrac{\pi}{2}, \dfrac{\pi}{2}\right]$ на $m$ равных частей. Разобьем $\left[-\dfrac{\pi}{2}, \dfrac{\pi}{2}\right]^{10}$ на гиперкубики с центрами $$(x_{i_1}, \ldots, x_{i_{10}}),\ i_k = 0, \ldots, m-1,\ k = 1, \ldots, 10$$ и мерой $h^{10}$ и воспользуемся методом прямоугольников
$$J \approx h^{10}\sum_{i_1=0}^{m-1}\ldots\sum_{i_{10}=0}^{m-1}f(x_{i_1}, \ldots, x_{i_{10}})$$

Для вычисления этой суммы потребуется $m^{10}$ вычислений функции $f$, что уже при $m = 5$ потребует около $10^7$ итераций. Для оптимизации вычислений воспользуемся симметричностью $f$:
$$J \approx \sum_{0 \leq i_1 \leq \ldots \leq i_{10} \leq m-1} \dfrac{10!}{n_0!\ldots n_{m-1}!}f(x_{i_1}, \ldots, x_{i_{10}}),$$
где $n_i$ обозначается число индексов, равных $i$ на данной итерации. Число слагаемых при этом равно ${m + 9 \choose 10}$. 

\paragraph{Оценка точности.} Для оценки погрешности будем использовать разложение в ряд Тейлора с остаточным членом в форме Лагранжа функции $f$:
$$f(y) = f(y_0) + \langle \nabla f(\xi), \ \Delta y\rangle,$$
где $y_0$ центр гиперкубика $\Delta$, a $\xi \in \Delta$. Интегрируя обе части по $\Delta$, получим, что ошибка интегрирования по каждому кубику равна
$$\int\limits_\Delta \langle \nabla f(\xi), \ \Delta y\rangle dy \leq \sup_\Delta \|\nabla f(x)\|\|\Delta y\|\mu(\Delta) \leq \dfrac{\sqrt{n}}{2}h^{n+1}\sup_{\left[-\frac{\pi}2, \frac{\pi}2\right]} \|\nabla f(x)\|,$$
а общая ошибка не превосходит $\dfrac{\pi^nh\sqrt{n}}{2}\sup\limits_{\left[-\frac{\pi}2, \frac{\pi}2\right]} \|\nabla f(x)\|$, то есть убывает линейно по $h$, однако высокая размерность требует для достижения высокой точности такого малого значения $h$, что вычисление квадратурной формулы с таким шагом не представляется возможным.
\subsection{Результаты работы программы}
\paragraph{Вычисление интеграла методом Монте--Карло.} Из полученных оценок точности для того, чтобы получить результат с точностью $\varepsilon = 0.05$ и надежностью $1 - \alpha = 0.9$ необходимо провести
$$n = \dfrac{\Var[f(Y)]}{\alpha\varepsilon^2} \approx 10^{10}$$
испытаний в методе Монте--Карло. Дисперсия здесь была оценена выборочной дисперсией, полученной на выборке размера $n = 10^7$. Интеграл вычисляется параллельно на $10^4$ выборках размера $10^{6}$ с последующим усреднением. Результат вычисления $J \approx 124.8225.$

Приведем также статистику десяти вычислений интеграла методом Монте--Карло на выборках размеров $10^6,\, 10^7,\, 10^8$ (рис. 18). Доверительный интервал с уровнем доверия $\alpha = 0.95$ получен из неравенства Чебышева. Среднее время вычисления оценивается медианой.

\begin{figure}[h]
	\center
	\includegraphics[scale=1]{6_1.pdf}
	\caption{Точность метода Монте--Карло.}
\end{figure} 

\paragraph{Вычисление интеграла методом квадратур.} С учетом всех оптимизаций, описанных в теоретической части, интегал был вычислен на сетке размера $30$ по каждому измерению, получен результат $J \approx 124.7040$, близкий к результату, полученному методом Монте--Карло.

\section{Задание 7}
\subsection{Постановка задачи}
\begin{enumerate}
	\item Методом случайного поиска найти минимальное значение функции $f$ на множестве $$A = \left\{(x_1, x_2)\colon \ x_1^2 + x_2^2 \leq 1\right\},$$ где 
 $$f(x_1, x_2) = x_1^3\sin\left(\dfrac{1}{x_1}\right) + 10x_1x_2^4 \cos\left(\dfrac{1}{x_2}\right),$$
 $f(0, 0) = 0$ по непрерывности.
	\item Методом имитации отжига найти минимальное значение функции Розенброка $g$ в пространстве $\mathbb{R}^2$, где
$$g(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2.$$
	\item Оценить точность. Сравнить результаты со стандартными методами оптимизации.
\end{enumerate}
\subsection{Теоретическая часть}
\paragraph{Метод случайного поиска.} Будем генерировать $n$ равномерно распределенных по окружности $A$ независимых случайных величин $X_1, \ldots X_n$ и выбирать в качестве решения ту, на которой минимизируется значение $f$.

Найдем распределение полярных координат при равномерном распределении по окружности:
$$\mathbb{P}(X \in B) = \dfrac{1}{\pi}\iint\limits_Bdxdy = \dfrac{1}{\pi}\iint\limits_Brdrd\varphi = \iint\limits_Bdr^2d\left(\dfrac{\varphi}{2\pi}\right),$$
откуда можно сделать вывод, что $\varphi \sim \mathcal{U}(0, 2\pi)$, а $r$ имеет функцию распределения
$$F_r(x) = 
\begin{cases}
0, &x < 0, \\
x^2, &x \in [0, 1], \\
1, &x > 1.
\end{cases}$$

Для моделирования $r$ будем использовать метод обращения функции распределения:
$$r \overset{d}{=} \sqrt{U}, \quad U \sim \mathcal{U}[0, 1].$$

\paragraph{Точность метода случайного поиска.}
Оценим точность реализованного метода. Пусть $x^* = \underset{x \in A}{\operatorname{argmax}}  f(x)$. Потребуем, чтобы для найденного решения $x$ выполнялось $|f(x^*) - f(x)| < \varepsilon$ с вероятностью $\alpha$. Так как фунцкция $f$ является непрерывно дифференцируемой в $A$, верна оценка
$$|f(x^*) - f(x)| \leq \|\nabla f(x^*)\| \|x - x^*\| \leq \sup_A \|\nabla f(x)\|\|x - x^*\| \leq \varepsilon$$
при $\|x - x^*\| \leq \delta = \dfrac{\varepsilon}{\sup\limits_A \|\nabla f(x)\|}$.

Оценим величину $\sup\limits_A \|\nabla f(x)\|$.
$$\dfrac{\partial f}{\partial x_1} = 3x_1^2\sin\left(\dfrac{1}{x_1}\right) - x_1\cos\left(\dfrac{1}{x_1}\right) + 10x_2^4\cos\left(\dfrac{1}{x_2}\right),$$
$$\dfrac{\partial f}{\partial x_2} = 40x_1x_2^3 \cos\left(\dfrac{1}{x_2}\right) + 10x_1x_2^2\sin\left(\dfrac{1}{x_2}\right),$$
$$\|\nabla f(x)\| = \sqrt{\left(\dfrac{\partial f}{\partial x_1}\right)^2 + \left(\dfrac{\partial f}{\partial x_2}\right)^2} \leq \sqrt{14^2 + 50^2} \leq 52.$$
Выберем число испытаний так, чтобы хотя бы один элемент выборки оказался в $\delta$-окрестности $x^*$ с вероятностью не менее $\alpha$. Учтем также, что $x^*$ может лежать на границе круга, и при малых $\delta$ окрестность $B_\delta(x^*) \bigcap A$ можно аппроксимировать полуокружностью.

$$\mathbb{P}(X_k \notin B_\delta(x^*), \ \ k = 1, \ldots, n) \leq \left(1 - \dfrac{\delta^2}{2}\right)^n \leq 1 - \alpha,$$
откуда $n = \dfrac{\ln(1 - \alpha)}{\ln\left(1 - \dfrac{\delta^2}{2}\right)}$.
\paragraph{Метод имитации отжига.}
Описание алгоритма.
\begin{enumerate}
	\item[0.] Задаются входные параметры $m, \sigma, t_0$ и начальное значение $(x_0, y_0)$.
	\item Генерируется кандидат на следующий шаг $(x_{k+1}, y_{k+1}) \sim (\mathcal{N}(x_k, t_k\sigma^2),\, \mathcal{N}(y_k, t_k\sigma^2))$ и вычиляется приращение функции $\Delta g = g(x_{k+1}, y_{k+1}) - g(x_k, y_k)$.
	\item В случае, если $\Delta g < 0$, делается шаг в $(x_{k+1}, y_{k+1})$.
В противном случае переход выполняется с вероятностью 
$p_k = \dfrac12\exp\left(-\dfrac{\Delta g}{t_k}\right)$.
	\item Понижается температура по закону
$$t_{k+1} = t_k m^{\frac{1}{1+0.1|\Delta g|}}.$$
Такое правило позволяет медленнее понижать температуру в случае, когда начальное приближение далеко от решения, и приращения функции велики.
\end{enumerate}

\subsection{Результаты работы программы}
\paragraph{Метод случайного поиска.}
Положим $\varepsilon = 0.01,\, \alpha = 0.99$. Тогда $\delta = \dfrac{0.01}{52}, \ n = 2.5 \cdot 10^8$.
При таких параметрах результат работы метода 
$$f_* \approx -1.2883, \quad x^* = (-0.3598, -0.9330).$$

\paragraph{Метод имитации отжига.}
Функция Розенброка достигает минимума в точке $x^* = (0, 0)$, где она принимает значение $f^* = 0$. Были подобраны следующие значения параметров в методе имитации отжига:
$$\sigma = 2.7, \quad m = 0.995, \quad t_0 = 2.$$

При таких значениях параметров алгоритм не совершает <<фатальных>> ошибок. Результат работы алгоритма, запущенного $1000$ раз из случайной точки, координаты которой имеют распределение $\mathcal{N}(0, 100)$, показан на рис. 19. Все решения концентрируются на прямой, проходящей через точное решение. Приведем также некоторую статистику, относящуюся к работе метода:
\begin{itemize}
	\item Среднее время работы: $0.116$ c.
	\item Средняя частота переходов: $0.7\%$.
	\item Среднее число итераций: $10163.5$.
\end{itemize}

Поскольку аналитическая оценка точности метода имитации отжига затруднительна, будем оценивать точность исходя из полученной выборки. На рис. 20 показаны распределения ошибки (расстояния от полученного результата до точного решения), а также ошибки по функционалу. Выборочные квантили порядка $0.99$ равны $0.0209$ и $0.0001$ соответственно.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{7_1.pdf}
	\caption{Результаты работы метода имитации отжига}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.5]{7_2.pdf}
	\hfill
	\includegraphics[scale=0.5]{7_3.pdf}
	\caption{Распределение величин $\|x - x^*\|$ и $\|f(x) - f(x^*)\|$.}
\end{figure}
 
\paragraph{Сравнение стохастических методов со стандартными методами оптимизации.} Применим метод COBYLA (Constrained Optimization BY Linear Approximation) к задаче из пункта 1. При начальном приближении $x_0 = (-0.2, -0.8)$ получаем результат, похожий на результат работы метода случайного поиска:
$$f^* = -1.2885, \quad x^* = (-0.3574, -0.9330).$$

Однако если взять начальное приближение, далекое от точного решения, например, $x_0 = (0.2, 0.1)$, метод дает неверный результат 
$$f^* = -0.0122, \quad x^* = (0.2455, -0.1110).$$

Метод сопряженных градиентов, выпущенный из $x_0 = (5, 5)$, дает точное решение задачи минимизации функции Розенброка:
$$f^* = 2 \cdot 10^{-12}, \quad x^* = (0.9999968, 0.9999972).$$
\section{Задание 8}
\subsection{Постановка задачи}
\begin{enumerate}
	\item Применить метод Монте--Карло к решению первой краевой задачи для двумерного уравнения Лапласа в единичном круге:
$$
\begin{cases}
\Delta u = 0, \quad (x, y) \in D = \{(x, y)\colon \ \ x^2 + y^2 \leq 1\}\\
u|_{\partial D} = f(x, y) \\
u \in C^2(D), \ f \in C(\partial D)
\end{cases}
$$

Для функции $f(x, y) = x^2 - y^2$ найти аналитическое решение и сравнить с полученным по методу Монте--Карло.
\end{enumerate}
\subsection{Теоретическая часть}
\paragraph{Применение метода Монте--Карло.}
Введем на плоскости сетку $x_{ij} = (-1 + ih, -1 + jh)$ с шагом $h$ и назовем внутренними узлы, для которых все 4 соседних узла лежат в круге. Узлы, лежащие в круге, но не удовлетворяющие этому условию, будем называть крайними. 

Аппроксимируя вторые частные производные, можно свести задачу Дирихле к следующей разностной схеме:
$$u_{ij} = \dfrac14(u_{i-1,j} + u_{i+1, j} + u_{i, j-1} + u_{i,j+1}) \text{ для внутренних узлов},$$
$$u_{ij} = f_{ij} \text{ для крайних узлов}.$$

Рассмотрим теперь следующую модель случайного блуждания: 
\begin{enumerate}
	\item Частица начинает свой путь во внутреннем узле $x_{ij}$.
	\item С равными вероятностями она переходит в один из четырех соседних узлов.
	\item Процесс продолжается до тех пор, пока частица не попадет на крайний узел $x_{pq}$.
\end{enumerate}


Обозначим $p(x_{ij}, x_{pq})$ вероятность, начав путь в узле $x_{ij}$, закончить его в узле $x_{pq}$. Из формулы полной вероятности следует
\begin{equation}\label{Dirich}
p(x_{ij}, x_{pq}) = \dfrac14(p(x_{i-1,j}, x_{pq}) + p(x_{i+1, j}, x_{pq}) + p(x_{i, j-1}, x_{pq}) + p(x_{i,j+1}, x_{pq}))
\end{equation}
для внутренних узлов $x_{ij}$ и $p(x_{ij}, x_{ij}) = 1$ для крайних узлов.

Рассмотрим теперь случайную величину $\xi_{ij} = f(x_{pq})$ --- значение $f$ в конечной точке пути.
Так как $\mathbb{E}\xi_{ij} = \sum_{p, q}p(x_{ij}, x_{pq})f(x_{pq})$, используя формулу (\ref{Dirich}) связи вероятностей на соседних узлах, получим
$$\mathbb{E}\xi_{ij} = \dfrac14(\mathbb{E}\xi_{i-1,j} + \mathbb{E}\xi_{i+1, j} + \mathbb{E}\xi_{i, j-1} + \mathbb{E}\xi_{i,j+1}) \text{ для внутренних узлов,}$$
$$\mathbb{E}\xi_{ij} = f_{ij} \text{ для крайних}.$$

Видно, что эти уравнения совпадают с разностной схемой, аппроксимирующей исходную задачу.

Значения $\mathbb{E}\xi_{ij}$ в каждом узле можно вычислить методом Монте--Карло, моделируя описанный выше случайных процесс. Данный метод подробно описан в \cite{Buslenko}.

Для оптимизации вычислений воспользуемся тем, что моделируемый процесс случайного блуждания обладает марковским свойством, поэтому при прохождении траектории через промежуточные узлы дальнейший путь частицы можно рассматривать как процесс, начавшийся в этом промежуточном узле.

\subsection{Результаты работы программы}

На рис. 21 показано решение, полученное методом Монте--Карло на сетке с шагом $h = 0.05$ при проведении $n = 10^4$ испытаний в каждой точке, а также аналитическое решение $f(x, y) = x^2 - y^2$. Максимальное отклонение от аналитического решения равно $0.0484$. Разность численного и аналитического решения показана на рис. 22.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{8_1.pdf}
	\hfill
	\includegraphics[scale=0.7]{8_2.pdf}
	\caption{Решение, полученное методом Монте--Карло, и аналитическое решение.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{8_3.pdf}
	\caption{Разность численного и аналитического решения при $n = 10^4$.}
\end{figure}

\newpage
\section{Задание 9}
\subsection{Постановка задачи}
Рассмотреть два вида процессов:
\begin{itemize}
	\item Винеровский процесс $W(t), t \in [0, 1], \ W(0) = 0.$
	\item Процесс Орнштейна--Уленбека $X(t),\ t \in [0, 1], \ X(0) = X_0$, то есть стационарный марковский гауссовский процесс. Начальное значение $X_0$ генерируется случайным образом так, чтобы полученный процесс бы стационарным.
\end{itemize}

Для данных процессов
\begin{enumerate}
	\item Найти ковариационную функцию и переходные вероятности.
	\item Промоделировать независимые траектории процесса с данными переходными вероятностями методом добавления разбиения отрезка.
	\item Построить график траекторий.
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Метод разбиения отрезка.}
Опишем кратко метод добавления разбиения отрезка. Пусть требуется смоделировать случайный процесс $X(t)$ на отрезке $[0, 1]$. 
\begin{enumerate}
	\item Моделирование $X_0 = X(0)$.
	\item Моделирование $X(1)$ по условному распределению $[X(1)|X(0)]$.
	\item Между каждыми двумя соседними узлами $t_i, t_{i + 1}$ сетки добавим узел $t = \dfrac{t_i + t_{i+1}}{2}$ и смоделируем процесс в нем, используя условное распределение случайной величины $$[X(t)|X(t_{i}) = x_i, X(t_{i+1}) = x_{i+1}].$$
	\item Повторение пункта 3 до тех пор, пока шаг сетки не станет достаточно мелким.
\end{enumerate}

\paragraph{Винеровский процесс.}
Винеровский процесс --- гауссовский процесс с независимыми приращениями такой, что
\begin{enumerate}
	\item $W(0) \overset{\text{п.н.}}{=} 0$
	\item $W(t + h) - W(t) \sim \mathcal{N}(0, \sigma^2h)$
\end{enumerate}

Найдем его ковариационную функцию функцию, предполагая $t_1 < t_2$:
\begin{multline}\notag
 R(t_1, t_2) = \mathbb{E}W(t_1)W(t_2) = \mathbb{E}W^2(t_1) + \mathbb{E}[W(t_1) - W(0)][W(t_2) - W(t_1)] =\\= \sigma^2 t_1 + \mathbb{E}W(t_1)\mathbb{E}[W(t_2) - W(t_1)] = \sigma^2 t_1,
\end{multline}
откуда
$$K(t_1, t_2) = \min(t_1, t_2) \sigma^2.$$

По определению $W(0) = 0, \ [W(1)|W(0)] \sim \mathcal{N}(0, \sigma^2)$. 

Найдем теперь для $t_1 < t_2 < t_3$ распределение $Y = [W(t_2)|W(t_1) = x_1, W(t_3) = x_3]$. 
Так как $W(t)$ имеет нормальное распределение с параметрами $0$ и $\sigma^2t$, $Y$ имеет плотность
$$p_Y(x_2) = \dfrac{p_{123}(x_1, x_2, x_3)}{p_{13}(x_1, x_3)},$$
где $p_{123}\A$ --- совместная плотность $(W(t_1), W(t_2), W(t_3)),\ $\ $p_{13}\A$ --- совместная плотность вектора $(W(t_1), W(t_3))$.

Известно, что плотность $n$-мерного нормального распределения с матожиданием $\mu$ и ковариационной матрицей $\Sigma$ имеет вид
\begin{equation}
p(x) = \dfrac{1}{\sqrt{(2\pi)^n|\Sigma|}}\exp\left(-\frac{1}{2} (x - \mu)^T\Sigma^{-1} (x - \mu)\right)
\end{equation}

Используя полученную для ковариационной функции формулу, получаем
$$
\Sigma_{13} = \sigma^2 \begin{pmatrix} 
t_1 & t_1 \\
t_1 & t_3
\end{pmatrix},
$$
$$
\Sigma_{123} = \sigma^2 \begin{pmatrix}
t_1 & t_1 & t_1 \\
t_1 & t_2 & t_2 \\
t_2 & t_2 & t_3
\end{pmatrix}.
$$

С помощью библиотеки для символьных вычислений Sympy найдем
$$p_Y(x_2) = C \exp\left\{ -\dfrac12\dfrac{4}{t_3 - t_1} \left(x_2 - \dfrac{x_1 + x_3}2\right)^2\right\},$$
то есть, обозначая шаг измельченной сетки за $h = t_2 - t_1$, получим $$Y \sim \mathcal{N}\left(\dfrac{x_1 + x_3}{2}, \dfrac{h\sigma^2}{2}\right).$$

\paragraph{Процесс Орнштейна--Уленбека.}

Процесс Орнштейна--Уленбека --- это стационарный марковский гауссовский процесс.
Известно \cite{Feller}, что для такого процесса 
$$\rho(X(t_1), X(t_3)) = \rho(X(t_1), X(t_2))\rho(X(t_2), X(t_3)), \quad t_1 < t_2 < t_3,$$
где $\rho\A$ --- коэффициент корреляции.

В силу однородности процесса для любого $t$ верно $X(t) \sim \mathcal{N}(0, \sigma^2)$ (без ограничения общности будем рассматривать центрированный процесс).
Найдем вид функции $K(t) = \rho(X(t), X_0)$, предполагая $0 \leq t_1 < t_2 \leq 1$:
$$ K(t_2) = \rho(X(t_2), X_0) = \rho(X(t_2), X(t_1))\rho(X(t_1), X_0) = K(t_2 - t_1)K(t_1).$$
То есть для любых $t, s \geq 0$ верно $K(t + s) = K(t)K(s)$. Логарифм этой функции $f(x) = \ln K(x)$ является решением уравнения Коши
$$f(x + y) = f(x) + f(y),$$
и, так как функция $K(t)$ непрерывна, она имеет вид
$K(t) = e^{-\lambda t}, \ \lambda > 0$, а ковариационная функция процесса
$$R(t_1, t_2) = \sigma^2 e^{-\lambda |t_2 - t_1|}, \quad \lambda > 0.$$

Сгенерируем $X_0$ как нормальную случайную величину с нулевым матожиданием и дисперсией $\sigma^2$.
Найдем теперь распределение $[X(1)|X_0]$.
Ковариационная матрица для $(X_0, X(1))$ имеет вид 
$$
\Sigma = \sigma^2 \begin{pmatrix} 
1 & e^{-\lambda} \\
e^{-\lambda} & 1
\end{pmatrix}.
$$

Проведя необходимые расчеты, получаем $$[X(1)|X_0] \sim \mathcal{N}\left(x_0e^{-\lambda}, \sigma^2 (1 -e^{-2\lambda})\right).$$

Найдем теперь распределение $Y = [X(t_2)|X(t_1) = x_1, X(t_3) = x_3],\ t_1 < t_2 < t_3$.

Обозначим шаг новой сетки за $h = t_2 - t_1 = t_3 - t_2$ и, как и в случае винеровского процесса, найдем ковариационные матрицы
$$
\Sigma_{13} = \sigma^2 \begin{pmatrix} 
1 & e^{-2\lambda h} \\
e^{-2 \lambda h} & 1
\end{pmatrix},
$$
$$
\Sigma_{123} = \sigma^2 \begin{pmatrix}
1 & e^{-\lambda h} & e^{-2 \lambda h} \\
e^{-\lambda h} & 1 & e^{-\lambda h} \\
e^{-2\lambda h} & e^{-\lambda h} & 1
\end{pmatrix}.
$$

После преобразований находим, что $$Y \sim \mathcal{N}\left(\dfrac{x_1 + x_3}{e^{\lambda h} + e^{-\lambda h}},\dfrac{\sigma^2 (e^{2\lambda h} - 1)}{e^{2\lambda h} + 1}\right).$$
\subsection{Результаты работы программы}

Все процессы смоделированы на сетке размера $N = 2^{15} + 1 \approx 3.3 \cdot 10^4$. Из ковариационной функции процесса Орнштейна--Уленбека видно, что с увеличением $\lambda$ корреляция между значениями быстро уменьшается, и процесс становится похож на белый шум. При  $\lambda = 0$ корреляция между любыми двумя значениями равна 1, и процесс является постоянной функцией $X_t \equiv X_0$.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{9_1.pdf}
	\caption{Моделирование стандартного винеровского процесса $W_t$.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{9_2.pdf}
	\caption{Моделирование процесса Орнштейна--Уленбека с $\lambda = 0.1,\, \sigma = 1$.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{9_3.pdf}
	\caption{Моделирование процесса Орнштейна--Уленбека с $\lambda = 10,\, \sigma = 1$.}
\end{figure}

\newpage
\section{Задание 10}
\subsection{Постановка задачи}
Произвести фильтрацию одномерного процесса Орнштейна--Уленбека:
\begin{enumerate}
	\item Используя генератор белого шума, добавить случайную ошибку с известной дисперсией к реализации процесса Орнштейна--Уленбека.
	\item При помощи одномерного фильтра Калмана оценить траекторию процесса по зашумленному сигналу. Параметры процесса и белого шума считать известными.
	\item Рассмотреть случай, когда шум
	\begin{itemize}
		\item является гауссовским;
		\item имеет распределение Коши.
	\end{itemize}
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Алгоритм работы фильтра Калмана.}
Рассмотрим сетку $$\{t_n = nh, n = 0, 1, \ldots, N\},\ h = \dfrac1N$$ и значения процесса Орнштейна-Уленбека
$x_n = X(t_n).$

Представим процесс в виде дискретной динамической системы вида
$$x_{n+1} = ax_n + \nu_n, \quad \mathrm{i.i.d.}\ \nu_n \sim \mathcal{N}(0, q), \quad x_1 \sim \mathcal{N}(0, \sigma^2).$$
Обозначим зашумленный сигнал
$$y_n = x_n + \varepsilon_n, \quad \mathrm{i.i.d.}\ \varepsilon_n \sim \mathcal{N}(0, r).$$
Параметры $\sigma, \lambda$ процесса считаем известными. Найдем через них параметры системы $a, q$. Для этого запишем ковариационную функцию для процесса в моменты $t_n$ и $t_{n+1}$:
$$R(t_n, t_n) = \Var(x_n) = \sigma^2,$$
$$R(t_n, t_{n+1}) = \mathrm{cov}(x_n, x_{n+1}) = a\ \Var(x_n) = a\sigma^2 = \sigma^2 e^{-\lambda h},$$
$$R(t_{n+1}, t_{n+1}) = \mathrm{var}(x_{n+1}) = a^2\Var(x_n) + q = a^2\sigma^2 + q = \sigma^2.$$
Отсюда легко выразить
$$a = e^{-\lambda h}, \quad q = \sigma^2(1 - e^{-2\lambda h}).$$

Опишем один шаг дискретного фильра Калмана. Обозначим за $\hat x_{n|n}$ оценку значения $x_n$ при известных $y_1, \ldots y_n$. $\hat x_{n|n-1}$ --- экстраполяция процесса на следующий шаг в соответствии с динамической системой. Через $p_{n|n}$ будем обозначать дисперсию ошибки фильтрации на $n$-м шаге, через $p_{n|n-1}$ --- прогнозируемую на следующем шаге дисперсию.

\begin{enumerate}
	\item Прогнозируем значение процесса и дисперсию ошибки
$$ \hat x_{n|n-1} = a\hat x_{n-1|n-1}, \quad p_{n|n-1} = a^2 p_{n-1|n-1} + q.$$
	\item Вычисляем разницу между наблюдаемым процессом и прогнозом и коэффициент усиления Калмана
$$ \delta_{n} = y_n - \hat x_{n|n-1}, \quad k_n = \dfrac{p_{n|n-1}}{p_{n|n-1} + r}.$$
	\item В качестве результата фильтрации берем выпуклую комбинацию наблюдаемого и предсказанного значения
$$ \hat x_{n|n} = \hat x_{n|n-1} + k_n\delta_n, \quad p_{k|k} = (1 - k_n)p_{n|n-1}. $$
\end{enumerate}

Дискретный фильтр Калмана минимизирует среднеквадратическую ошибку в классе линейных оценок. Доказательство этого факта можно найти, например, в \cite{Ostrem}.

Поскольку рассматривается линейный фильтр гауссовского процесса с гауссовским шумом, ошибка фильтрации будет иметь нормальное распределение, а доверительный интервал с уровнем доверия $\alpha$ будет иметь вид
$$[\hat x_{n|n} - \Delta, \, \hat x_{n|n} + \Delta], \quad \Delta = -\sqrt{p_{n|n}}\Phi^{-1}\left(\dfrac{\alpha}2\right).$$
\subsection{Результаты работы программы}

\paragraph{Нормально распределенная ошибка.} Всюду в этом задании рассматриваем процесс Орнштейна--Уленбека с $\lambda = 0.01, \, \sigma = 1$. Добавим к нему нормальный шум с дисперсией $r = 0.01$. (рис. 26). Результат фильтрации показан на рис. 27, а доверительные интервалы с уровнем доверия $\alpha = 0.95$ на рис. 28. При этом вне интервалов процесс оказывается в $3.19\%$ времени.


\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{10_1.pdf}
	\caption{Ошибка распределена нормально, $r = 0.01$.}
	\includegraphics[scale=0.7]{10_2.pdf}
	\caption{Результат работы фильтра Калмана.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{10_3.pdf}
	\caption{Доверительные интервалы.}
\end{figure}

\paragraph{Ошибка имеет распределение Коши.} При добавлении шума, имеющего распределение Коши с нулевым параметром сдвига и с параметром масштаба, равным $0.0001$, в зашумленном сигнале появляются сильные выбросы, возникающие из-за тяжелых хвостов распределения Коши (рис. 29). Эти же выбросы иногда сохраняются и после фильтрации (рис. 30), и в доверительных интервалах (рис. 31). Вне доверительного интервала порядка $\alpha = 0.95$ опказывается $1.71\%$ траектории.

\newpage

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{10_4.pdf}
	\caption{Ошибка имеет распределение Коши с параметром масштаба $10^{-4}$.}
\end{figure}
\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{10_5.pdf}
	\caption{Результат работы фильтра Калмана.}
\end{figure}

\begin{figure}[h]
	\center
	\includegraphics[scale=0.7]{10_6.pdf}
	\caption{Доверительные интервалы.}
\end{figure}

\newpage
\section{Задание 11}
\subsection{Постановка задачи}
Построить двумерное пуассоновское поле, отвечающее сложному пуассоновскому процессу:
\begin{enumerate}
	\item Первая интерпретация: система массового обслуживания. При этом, первая координата поля --- время поступления заявки в СМО (равномерное распределение), вторая --- время ее обслуживания (распределение $\chi^2$ с 10 степенями свободы).
	\item Вторая интерпретация: система массового обслуживания с циклической активностью $\lambda(1 + \cos(t))$ и единичными скачками. Свести данную задачу при помощи метода Льюиса и Шадлеара к моделированию двумерного пуассоновского поля, где первая координата имеет равномерное распределение, а вторая --- распределение Бернулли.
	\item Третья интерпретация: работа страховой компании. Первая координата --- момент наступления страхового случая (равномерное распределение), вторая координата --- величина ущерба (распределение Парето). Поступление капитала по времени линейно со скоростью $c > 0$, начальный капитал $W > 0$.
	\item Для каждой системы рассмотреть все возможные случаи поведения системы в зависимости от значения параметров.
\end{enumerate}

\subsection{Теоретическая часть}
\paragraph{Моделирование СМО.}
Будем моделировать систему массового обслуживания на отрезке времени $[0, T]$. Обозначим $\eta \sim \mathrm{Pois}(\lambda T)$ число заявок, поступивших в систему за время от $0$ до $T$. $\lambda$ --- интенсивность поступления заявок, то есть среднее число новых заявок в единицу времени.

Пусть $t_1, \ldots, t_\eta$ --- времена поступления заявок, $t_j \sim \mathcal{U}[0, T],\ $ $t_{(j)}$ - $j$-я порядковая статистика, $s_j \sim \chi^2(10)$ --- время обработки $j$-й заявки (то есть заявки, поступившей в момент $t_{(j)}$).

Обозначим $X_j$ время окончания обработки $j$-й заявки. Заметим, что
$$X_1 = t_{(1)} + s_1, \quad 
X_j = \begin{cases}
t_{(j)} + s_j, & X_{j - 1} \leq t_{(j)} \\
X_{j-1} + s_j, & X_{j - 1} > t_{(j)}
\end{cases}
$$

Таким образом, число заявок в очереди в момент времени $t$ можно выразить следующим образом:
$$N(t) = \sum_{j=1}^{\eta}\I_{t_{(j)} \leq t < X_j},$$
то есть как число заявок, которые уже поступили к моменту $t$, но еще не были обработаны.

Будем моделировать $\eta \sim \mathrm{Pois}(\lambda T)$, затем $t_1, \ldots, t_\eta \sim \mathcal{U}[0, 1],\ s_1, \ldots, s_\eta \sim \chi^2(10)$ как независимые случайные величины. Затем на основе полученной выборки вычисляются $X_1, \ldots, X_\eta$ и $N(t)$.

Заметим также, что время между заявками имеет экспоненциальное распределение с параметром $\lambda$, то есть среднее время между заявками равно $\dfrac{1}{\lambda}$. Среднее время обработки одной заявки равно $\mathbb{E}s_1 = 10$. Таким образом, если $\dfrac{1}{\lambda} < 10$, то есть $\lambda > \dfrac{1}{10}$, то заявки поступают с большей скоростью, чем успевают обрабатываться, и длина очереди в среднем растет.
Если $\lambda < \dfrac{1}{10}$, то скорость обработки превышает скорость поступления. $\lambda = \dfrac{1}{10}$ соответствует некоторому промежуточному состоянию,
при котором скорость поступления заявок совпадает со скоростью их обработки.

\paragraph{Моделирование СМО с циклической активностью.}
Для моделировния СМО с циклической интенсивностью $\lambda(t) = \lambda(1 + \cos(t))$ будем использовать метод Льюиса и Шедлеара (его обоснование можно найти в \cite{Lewis}). Так как $\lambda(t) \leq 2\lambda$ для любых $t$, будем использовать однородный пуассоновский процесс с интенсивностью $\lambda^* = 2 \lambda$. Опишем алгоритм моделирования.

\begin{enumerate}
	\item Генерируем $\eta \sim \mathrm{Pois}(\lambda^*)$ --- число поступивших заявок --- и $t_1^*, \ldots, t_\eta^* \sim \mathcal{U}[0, T]$ --- времена поступления заявок в процессе $N^*(\cdot)$.
	\item Для каждого $j$ удаляем $t_j^*$ из выборки с вероятностью $1 - \dfrac{\lambda(t_j^*)}{\lambda^*(t_j^*)}$.
	\item Оставшиеся точки являются моментами поступления заявки в процессе $N(\cdot)$.
\end{enumerate}

Так как средняя мгновенная интенсивность процесса равна $\lambda$, сценарии функционирования СМО аналогичны пункту 1.

\paragraph{Моделирование работы страховой компании.}
Пусть число страховых случаев к моменту $t$ равно $N(t)$ --- однородный пуассоновский процесс с интенсивностью $\lambda$. Тогда капитал страховой компании изменяется по закону
$$K(t) = W + ct - \sum_{k=1}^{N(t)}\xi_k,$$
где $\xi_k$ имеет распределение Парето с плотностью $p_\xi(x) = \dfrac{k}{x^{k+1}}, \ x > 1$. Параметр распределения Парето $k > 0$.

Если в какой-то момент $\tau$ капитал $K(\tau)$ достигает нуля, то $K(t) = 0, \ t > \tau$.

Алгоритм моделирования аналогичен предыдущим пунктам: генерируется $\eta \sim \mathrm{Pois}(\lambda T)$,
$t_1, \ldots, t_\eta \sim \mathcal{U}[0, T]$. Случайные величины $\xi_1, \ldots, \xi_\eta$, имеющие распределение Парето с параметром $k$, будем моделировать методом обращения функции распределения:
$$\xi_j = (1 - U_j)^{-\frac{1}{k}}, \quad U_j \sim \mathcal{U}[0, 1], \quad j = 1, \ldots, \eta.$$
Далее по вышеописанным формулам вычисляется капитал $K(t)$.

При $k > 1$ распределение Парето имеет математическое ожидание, равное $\dfrac{k}{k-1}$. Тогда легко показать, что 
$$\mathbb{E}K(t) = W + ct - \lambda t \dfrac{k}{k-1},$$
то есть при $c - \dfrac{\lambda k}{k-1} > 0$ капитал компании в среднем будет расти. При противоположном неравенстве K(t) в среднем будет убывать, приводя к разорению. 

При $k \leq 1$ математическое ожидание $\xi_j$ равно бесконечности, и на достаточно большом промежутке времени компания  будет разоряться при любых значениях $c, W, \lambda > 0$.
\subsection{Результаты работы программы}

\paragraph{Моделирование СМО.} На рис. 32-34 показаны результаты моделирования СМО с параметрами $\lambda = 0.1,\, 0.05,\, 0.2$ соответственно, что соответствует трем основным сценариям, описанным в теоретической части.

На рис. 35 продемонстрирована работа СМО с циклической интенсивностью на отрезке $[0,\, 4\pi]$ с параметром $\lambda = 10$. Видно, что в моменты, когда интенсивность близка к нулю, новых заявок практически не поступает.

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_1.pdf}
	\caption{Работа СМО с $\lambda = 0.1$}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_2.pdf}
	\caption{Работа СМО с $\lambda = 0.05$}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_3.pdf}
	\caption{Работа СМО с $\lambda = 0.2$}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_4.pdf}
	\caption{Работа СМО с циклической интенсивностью. $\lambda = 10$}
\end{figure}

\paragraph{Моделирование работы страховой компании.} На рис. 36-38 показаны сценарии работы страховой компании на отрезке $[0, 100]$. Параметры модели $W = 30,\, k = 2,\, c = 2$.  Параметр $\lambda$ изменяется так, чтобы охватить все три описанных в теоретической части случая.

На рис. 39 показано, что при $k = 1$, когда распределение Парето не имеет математического ожидания, один страховой случай может полностью разорить фирму.

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_5.pdf}
	\caption{Моделирование работы страховой компании с $W = 30,\, k = 2,\, c = 2,\, \lambda = 1$.}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_6.pdf}
	\caption{Моделирование работы страховой компании с $W = 30,\, k = 2,\, c = 2,\, \lambda = 1.1$.}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_7.pdf}
	\caption{Моделирование работы страховой компании с $W = 30,\, k = 2,\, c = 2,\, \lambda = 0.9$.}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=0.7]{11_8.pdf}
	\caption{Моделирование работы страховой компании с $W = 100,\, k = 1,\, c = 8,\, \lambda = 1$.}
\end{figure}
\newpage
\center
\begin{thebibliography}{0}
\bibitem{Smirnov}
Смирнов, С.Н., Лекции по курсу <<Стохастический анализ и моделирование>>, 2020.
\bibitem{DeGroot}
DeGroot, M.H. \& M.J. Schervish (2011), Probability and Statistics, 4th Ed, Pearson.
\bibitem{Lewis}
Lewis, P. A. W., \& Shedler, G. S. (1979). Simulation of nonhomogeneous poisson processes by thinning. Naval Research Logistics Quarterly, 26(3), 403–413. doi:10.1002/nav.3800260304 
\bibitem{Kolm} 
Kolmogorov A (1933), Sulla determinazione empirica di una legge di distribuzione. G. Ist. Ital. Attuari. 4: 83–91.
\bibitem{Smir}
Smirnov, N. (1939), On the estimation of the discrepancy between empirical curves of distribu- tion for two independent samples. Bull. Math. Univ. Moscou 2:2.
\bibitem{Buslenko}
Бусленко Н.П., Голенко Д.И., Соболь И.М., Срагович В.Г., Шрейдер Ю.А. Метод статистических испытаний. 1962. 332 с. Букинист. 
\bibitem{Ostrem}
Острем К.Ю. Введение в стохастическую теорию управления. М.: Мир, 1973. --- 324 с.
\bibitem{Feller}
Феллер В. Введение в теорию вероятностей и ее приложения (том 2). М.: Мир, 1967 г. – 765 с.
\bibitem{Shir}
Ширяев А. Н. Вероятность, — М.: Наука. 1989. 
\end{thebibliography}

\end{document}
